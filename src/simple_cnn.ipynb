{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN\n",
    "\n",
    "\n",
    "Simple implementation of a CNN on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/t/tsob/Documents/cs231n/proj/venv/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# A bit of setup\n",
    "\n",
    "# Usual imports\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Notebook plotting magic\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# For auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Deep learning related\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "# My modules\n",
    "import generate_data as d\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" Returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8. np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(num=5):\n",
    "    \"\"\"\n",
    "    Load a bit of data from SALAMI.\n",
    "    Argument: num (number of songs to load. Default=5)\n",
    "    Returns: X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \"\"\"\n",
    "    X, y = d.get_data(num)\n",
    "\n",
    "    # Keep last 6000 data points for test\n",
    "    X_test, y_test = X[-6000:], y[-6000:]\n",
    "    X_train, y_train = X[:-6000], y[:-6000]\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # Make column vectors\n",
    "    y_train = y_train[:,np.newaxis]\n",
    "    y_val   = y_val[:,np.newaxis]\n",
    "    y_test  = y_test[:,np.newaxis]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \"\"\"\n",
    "    Build the CNN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make an input layer\n",
    "    network = lasagne.layers.InputLayer(\n",
    "        shape=(\n",
    "            None,\n",
    "            1,\n",
    "            20,\n",
    "            515\n",
    "            ),\n",
    "        input_var=input_var\n",
    "        )\n",
    "\n",
    "    # Add a conv layer\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        network,           # Incoming\n",
    "        num_filters=32,    # Number of convolution filters to use\n",
    "        filter_size=(5, 5),\n",
    "        stride=(1, 1),     # Stride fo (1,1)\n",
    "        pad='same',        # Keep output size same as input\n",
    "        nonlinearity=lasagne.nonlinearities.rectify, # ReLU\n",
    "        W=lasagne.init.GlorotUniform()   # W initialization\n",
    "        )\n",
    "\n",
    "    # Apply max-pooling of factor 2 in second dimension\n",
    "    network = lasagne.layers.MaxPool2DLayer(\n",
    "        network, pool_size=(1, 2)\n",
    "        )\n",
    "    # Then a fully-connected layer of 256 units with 50% dropout on its inputs\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify\n",
    "        )\n",
    "    # Finally add a 1-unit softmax output layer\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        network,\n",
    "        num_units=1,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax\n",
    "        )\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iteration\n",
    "\n",
    "This is just a simple helper function iterating over training data in\n",
    "mini-batches of a particular size, optionally in random order. It assumes\n",
    "data is available as numpy arrays. For big datasets, you could load numpy\n",
    "arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "own custom data iteration function. For small datasets, you can also copy\n",
    "them to GPU at once for slightly improved performance. This would involve\n",
    "several changes in the main program, though, and is not demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    \"\"\"\n",
    "    Generate a minibatch.\n",
    "    Arguments: inputs    (numpy array)\n",
    "               targets   (numpy array)\n",
    "               batchsize (int)\n",
    "               shuffle   (bool, default=False)   \n",
    "    Returns:   inputs[excerpt], targets[excerpt]\n",
    "    \"\"\"\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano config\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "SID: 1566\n",
      "SID: 1574\n",
      "SID: 1504\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18759, 1, 20, 515)\n",
      "(18759, 1)\n",
      "(10000, 1, 20, 515)\n",
      "(10000, 1)\n",
      "(6000, 1, 20, 515)\n",
      "(6000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions\n",
    "for datapt in [X_train, y_train, X_val, y_val, X_test, y_test]:\n",
    "    print datapt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train  = 18759\n",
      "n_val    = 10000\n",
      "n_test   = 6000\n",
      "n_chan   = 1\n",
      "n_feats  = 20\n",
      "n_frames = 515\n"
     ]
    }
   ],
   "source": [
    "# Parse dimensions\n",
    "n_train  = y_train.shape[0]\n",
    "n_val    = y_val.shape[0]\n",
    "n_test   = y_test.shape[0]\n",
    "n_chan   = X_train.shape[1]\n",
    "n_feats  = X_train.shape[2]\n",
    "n_frames = X_train.shape[3]\n",
    "\n",
    "print \"n_train  = {0}\".format(n_train)\n",
    "print \"n_val    = {0}\".format(n_val)\n",
    "print \"n_test   = {0}\".format(n_test)\n",
    "print \"n_chan   = {0}\".format(n_chan)\n",
    "print \"n_feats  = {0}\".format(n_feats)\n",
    "print \"n_frames = {0}\".format(n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var  = T.tensor4( name='inputs' )\n",
    "target_var = T.fcol( name='targets' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions... Done.\n"
     ]
    }
   ],
   "source": [
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\"),\n",
    "network = build_cnn(input_var)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want to minimize\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create update expressions for training\n",
    "# Here, we'll use adam\n",
    "params  = lasagne.layers.get_all_params(\n",
    "    network,\n",
    "    trainable=True\n",
    ")\n",
    "updates = lasagne.updates.adam(\n",
    "    loss,\n",
    "    params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for validation/testing.\n",
    "# The crucial difference here is that we do a deterministic forward pass\n",
    "# through the network, disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function(\n",
    "    [input_var, target_var],\n",
    "    loss,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function(\n",
    "    [input_var, target_var],\n",
    "    [test_loss, test_acc],\n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 1 took 350.628s\n",
      "  training loss:\t\t0.899833\n",
      "  validation loss:\t\t0.869502\n",
      "  validation accuracy:\t\t39.40 %\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "print(\"Done training.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.855475\n",
      "  test accuracy:\t\t42.17 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "# Optionally, you could now dump the network weights to a file like this:\n",
    "# np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "#\n",
    "# And load them again later on like this:\n",
    "# with np.load('model.npz') as f:\n",
    "#     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "# lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_params = lasagne.layers.get_all_param_values(network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
